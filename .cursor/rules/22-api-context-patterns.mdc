---
globs: app/(chat)/api/**/*.ts
description: "API patterns for efficient context management and conversation persistence"
---

# API Context Management Patterns

## Overview

Essential patterns for implementing efficient context management in chat and compare API endpoints, ensuring conversation continuity while optimizing token usage.

## Context Loading Patterns

### Load and Combine Messages

```typescript
// Standard pattern for loading conversation history
const dbMessages = await getMessagesByChatId({ id: chatId });

// Create user message for current request
const userMessage = {
  id: generateUUID(),
  role: "user" as const,
  parts: [{ type: "text" as const, text: prompt }],
  metadata: { createdAt: new Date().toISOString() },
};

// Combine existing + new message
const uiMessages = [...convertToUIMessages(dbMessages), userMessage];

// Save user message immediately for conversation continuity
await saveMessages({
  messages: [
    {
      chatId,
      id: userMessage.id,
      role: "user",
      parts: userMessage.parts,
      attachments: [],
      createdAt: new Date(),
    },
  ],
});
```

## Model-Specific Context Filtering

### Filter Assistant Messages by Model

```typescript
const buildModelSpecificContext = (
  targetModelId: string,
  uiMessages: any[]
) => {
  // Critical: Only include relevant context for each model
  const filteredUIMessages = uiMessages.filter((msg) => {
    if (msg.role === "user") return true; // All user messages
    if (msg.role === "assistant") {
      // Only this model's previous responses
      return msg.metadata?.modelId === targetModelId;
    }
    return true; // Other message types
  });

  // Apply token optimization
  const contextOptimizedMessages = optimizeContextWindow(filteredUIMessages);
  return convertToModelMessages(contextOptimizedMessages) as any[];
};
```

## Dynamic Context Optimization

### Token-Based Window Management

```typescript
const optimizeContextWindow = (messages: any[]) => {
  const OPTIMAL_CONTEXT_TOKENS = 2000;
  const MIN_RECENT_PAIRS = 2;
  const MAX_RECENT_PAIRS = 5;

  // Group into conversation pairs with token estimates
  const pairs = groupMessageIntoPairs(messages);
  const totalTokens = pairs.reduce((sum, pair) => sum + pair.tokenEstimate, 0);

  // Return all if within limits
  if (totalTokens <= OPTIMAL_CONTEXT_TOKENS) {
    return messages;
  }

  // Select optimal recent context
  return selectRecentPairsWithinTokenBudget(pairs, OPTIMAL_CONTEXT_TOKENS);
};

// Helper functions
const estimateTokens = (text: string): number => Math.ceil(text.length / 4);
const getMessageText = (message: any): string => {
  return (
    message.parts
      ?.filter((p) => p.type === "text")
      .map((p) => p.text)
      .join(" ") || ""
  );
};
```

## Response Persistence Patterns

### Individual Model Response Storage

```typescript
// After compare run completion - save each model's response separately
for (const result of completedRun.results) {
  if (result.status === "completed" && result.content) {
    const modelSpecificMessage = {
      id: generateUUID(),
      role: "assistant" as const,
      parts: [
        // Include reasoning if present
        ...(result.reasoning
          ? [{ type: "reasoning" as const, text: result.reasoning }]
          : []),
        // Main content without model attribution (metadata handles this)
        { type: "text" as const, text: result.content },
      ],
      metadata: {
        compareRunId: runId,
        modelId: result.modelId, // Critical: Track source model
        createdAt: new Date().toISOString(),
      },
    };

    await saveMessages({
      messages: [
        {
          chatId,
          id: modelSpecificMessage.id,
          role: "assistant",
          parts: modelSpecificMessage.parts,
          attachments: [],
          createdAt: new Date(),
        },
      ],
    });
  }
}
```

## Error Handling Patterns

### Graceful Context Fallback

```typescript
try {
  const dbMessages = await getMessagesByChatId({ id: chatId });
  const uiMessages = [...convertToUIMessages(dbMessages), userMessage];
  const contextOptimizedMessages = optimizeContextWindow(uiMessages);
  allMessages = convertToModelMessages(contextOptimizedMessages) as any[];
} catch (e) {
  console.warn("Failed to load/optimize context, using fallback:", e);
  // Fallback: just use current prompt
  allMessages = [{ role: "user", content: prompt }];
  uiMessagesWithMetadata = [
    { role: "user", parts: [{ type: "text", text: prompt }] },
  ];
}
```

### Context Building Validation

```typescript
// Validate context before sending to model
const validateContext = (messages: any[]): boolean => {
  if (!messages || messages.length === 0) return false;

  // Ensure we have at least one user message
  const hasUserMessage = messages.some((msg) => msg.role === "user");
  if (!hasUserMessage) return false;

  // Check token limits
  const totalTokens = messages.reduce(
    (sum, msg) => sum + estimateTokens(msg.content || ""),
    0
  );
  if (totalTokens > 4000) return false; // Model context limit

  return true;
};
```

## Usage in API Endpoints

### Compare Stream API Pattern

```typescript
// app/(chat)/api/compare/stream/route.ts
export const POST = authenticatedRoute(async (request, _context, user) => {
  const { chatId, prompt, modelIds } = requestBody;

  // 1. Load and prepare context
  let uiMessagesWithMetadata: any[] = [];
  try {
    const dbMessages = await getMessagesByChatId({ id: chatId });
    const userMessage = createUserMessage(prompt);
    uiMessagesWithMetadata = [...convertToUIMessages(dbMessages), userMessage];
    await saveMessages({ messages: [userMessage] });
  } catch (error) {
    // Handle context loading errors...
  }

  // 2. Process each model with filtered context
  const modelPromises = modelIds.map(async (modelId: string) => {
    const modelSpecificMessages = buildModelSpecificContext(
      modelId,
      uiMessagesWithMetadata
    );

    return streamText({
      model: getLanguageModel(modelId),
      messages: modelSpecificMessages, // Optimized context
      // ... other options
    });
  });

  // 3. Save responses after completion
  Promise.all(modelPromises).then(async () => {
    await saveIndividualModelResponses(completedRun);
  });
});
```

### Regular Chat API Pattern

```typescript
// app/(chat)/api/chat/route.ts
export const POST = authenticatedRoute(async (request, _context, user) => {
  const { id, message } = requestBody;

  // Load context and combine with new message
  const messagesFromDb = await getMessagesByChatId({ id });
  const uiMessages = [...convertToUIMessages(messagesFromDb), message];

  // Save user message
  await saveMessages({ messages: [userMessageForDb] });

  // Use full context for single model (no filtering needed)
  const result = streamText({
    model: getLanguageModel(effectiveModel),
    messages: convertToModelMessages(uiMessages),
    // ... options
  });

  // Auto-save all messages on completion
  result.onFinish = async ({ messages }) => {
    await saveMessages({ messages: formatMessagesForDb(messages) });
  };
});
```

## Performance Considerations

### Context Caching

```typescript
// Cache processed context to avoid recomputation
const contextCache = new Map<string, any[]>();

const getCachedContext = (cacheKey: string, messages: any[]) => {
  if (contextCache.has(cacheKey)) {
    return contextCache.get(cacheKey);
  }

  const optimized = optimizeContextWindow(messages);
  contextCache.set(cacheKey, optimized);
  return optimized;
};
```

### Memory Management

```typescript
// Clean up context cache periodically
setInterval(() => {
  if (contextCache.size > 100) {
    contextCache.clear();
  }
}, 300000); // Every 5 minutes
```

## Testing Context Management

```typescript
// Test context filtering
test("model-specific context excludes competitor responses", () => {
  const messages = [
    { role: "user", content: "Hello" },
    {
      role: "assistant",
      content: "GPT response",
      metadata: { modelId: "gpt-4" },
    },
    {
      role: "assistant",
      content: "Claude response",
      metadata: { modelId: "claude-3" },
    },
  ];

  const gptContext = buildModelSpecificContext("gpt-4", messages);
  expect(gptContext.some((msg) => msg.content?.includes("GPT response"))).toBe(
    true
  );
  expect(
    gptContext.some((msg) => msg.content?.includes("Claude response"))
  ).toBe(false);
});

// Test token optimization
test("context optimization stays within token limits", () => {
  const longConversation = generateLongConversation(50);
  const optimized = optimizeContextWindow(longConversation);
  const tokenCount = optimized.reduce(
    (sum, msg) => sum + estimateTokens(msg.content),
    0
  );

  expect(tokenCount).toBeLessThan(2500);
  expect(optimized.length).toBeGreaterThan(4); // Minimum context preserved
});
```

## Critical Implementation Notes

### Database Schema Requirements

```sql
-- Ensure messages table supports metadata for model tracking
ALTER TABLE messages ADD COLUMN IF NOT EXISTS metadata JSONB;
CREATE INDEX IF NOT EXISTS idx_messages_metadata_model_id ON messages USING GIN ((metadata->>'modelId'));
```

### Message Metadata Structure

```typescript
interface MessageMetadata {
  modelId?: string; // For assistant messages from compare mode
  compareRunId?: string; // Link to compare run
  createdAt: string; // ISO timestamp
  isSummary?: boolean; // For future summary messages
}
```

These patterns ensure efficient, scalable context management across all chat and compare API endpoints while maintaining conversation continuity and optimizing token usage.
